{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Deep_SAGA_V2021_vGit.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OliDeane/Deep_SAGA/blob/master/Deep_SAGA_V2021_vGit.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jZiG5TyVx23D"
      },
      "source": [
        "**Before Running:**\n",
        "\n",
        "Create a folder in Google drive named 'Gaze_Detection_System' and store the video being processed and the associated gaze data in that folder. Also in this folder should be the coco_labels.txt file, maskrcnn_predict.py and mask_rcnn_coco.h5. The folder should be made in myDrive so the path should look like this: myDrive > Gaze_Detection_System.\n",
        "\n",
        "Name the video in the correct format (e.g. pp1_s1_City_vidA.mp4). Name the gaze data in the correct format (e.g. GD_pp1_s1_City_vidA). Bare in mind that both are case sensitive. \n",
        "\n",
        "There is an example video and gaze data in the shared google drive - use this to test that the system is working - (for this, insert 1 for the pp number input box, 1 for the session number box, 'City' for the environmnet type and A for the video box.\n",
        "\n",
        "Also ensure that the GPU is activated for your Google Colab environmnet. Go to 'Runtime' > 'Change Runtime Type' > Select 'GPU' > 'Save'.\n",
        "\n",
        "**After Running:**\n",
        "\n",
        "When prompted insert the pp number, vid number, session number, environmnet type and date. \n",
        "\n",
        "The code will also ask whether you would like to check how the greenery detector is identifying green within the video. If it is the first time running the code for a given video, then it is advised that you respond 'yes' to this and follow the instructions at the 'greenery detection' section of this code.\n",
        "**bold text**\n",
        "You will also have to mount the google drive. When prompted, follow the provided link, proceeed to your google account and copy and paste the new link into the provided input box. \n",
        "\n",
        "Outputs:\n",
        "\n",
        "1) List of what is being gazed upon for each frame.\n",
        "2) Summary sheet informing on the total number of frames processed, the average number of persons and vehicles appearing in a frame and the average percentage of pixels within a frame that were tagged as green.\n",
        "3) Graph visualising what is being gazed upon for each frame.\n",
        "4) Heatmap displaying distribution of gaze\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kMc6nWqjuP6V",
        "outputId": "86c9946e-e6aa-42e3-eb17-50345c8fa2cf"
      },
      "source": [
        "! git clone https://github.com/OliDeane/Deep_SAGA.git"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'Deep_SAGA'...\n",
            "remote: Enumerating objects: 68, done.\u001b[K\n",
            "remote: Counting objects: 100% (68/68), done.\u001b[K\n",
            "remote: Compressing objects: 100% (48/48), done.\u001b[K\n",
            "remote: Total 68 (delta 24), reused 31 (delta 15), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (68/68), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XzJAZbugzJaZ"
      },
      "source": [
        "!git config --global user.email \"oliver.deane@gmail.com\"\n",
        "!git config --global user.name \"OliDeane\""
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "khFS2FBz5gmu",
        "outputId": "2a31d9c1-d59a-4546-f30b-5b19ecca595d"
      },
      "source": [
        "!git push origin master"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: could not read Username for 'https://github.com': No such device or address\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "97pGJJK1LPr2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130
        },
        "outputId": "06fb12c8-007a-4bd8-c195-b77afe826647"
      },
      "source": [
        "|# Connect to Github\n",
        "\n",
        "# Mount the google drive\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive\")\n",
        "\n",
        "# Go to the project folder and clone the github repo into the project folder\n",
        "cd gdrive/My Drive/Deep_SAGA\n",
        "! git clone https://github.com/OliDeane/Deep_SAGA.git\n",
        "cd Deep_SAGA/System_Code/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-7feeb8f756fa>\"\u001b[0;36m, line \u001b[0;32m8\u001b[0m\n\u001b[0;31m    cd gdrive/My Drive/Deep_SAGA\u001b[0m\n\u001b[0m            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ApdpDwL-HEu-"
      },
      "source": [
        "# Import all the main packages\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import colorsys\n",
        "import argparse\n",
        "import imutils\n",
        "import random\n",
        "import cv2\n",
        "import os\n",
        "import time\n",
        "from google.colab.patches import cv2_imshow\n",
        "from google.colab import files\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "from datetime import datetime\n",
        "import io\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import h5py\n",
        "if h5py.__version__ != '2.10.0\":\n",
        "  !pip install h5py==2.10.0\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Urm3LXYvq7Oq",
        "outputId": "25e4746a-e22a-4ea2-f844-b11e24b4c8ee"
      },
      "source": [
        "# Go to correct file location!\n",
        "# !ls\n",
        "# !cd Deep_SAGA/System_Code/\n",
        "\n",
        "\n",
        "%cd Deep_SAGA/System_Code\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/Deep_SAGA/System_Code\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ssv2W5p2GcMG",
        "outputId": "f37bd2b2-0bde-4a7a-8d49-b28ba4cf870c"
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "import tensorflow as tf\n",
        "\n",
        "#Check that tensorflow is running using the GPU\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))\n",
        "print(tf.__version__)\n",
        "\n",
        "# Grab the MRCNN model from github\n",
        "!pip install git+https://github.com/matterport/Mask_RCNN.git\n",
        "\n",
        "#Import the mrcnn model and all necessary accompanying files\n",
        "import mrcnn\n",
        "from mrcnn.config import Config\n",
        "from mrcnn import model as modellib\n",
        "from mrcnn import visualize"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n",
            "Found GPU at: /device:GPU:0\n",
            "1.15.2\n",
            "Collecting git+https://github.com/matterport/Mask_RCNN.git\n",
            "  Cloning https://github.com/matterport/Mask_RCNN.git to /tmp/pip-req-build-hc4ux6ki\n",
            "  Running command git clone -q https://github.com/matterport/Mask_RCNN.git /tmp/pip-req-build-hc4ux6ki\n",
            "Requirement already satisfied (use --upgrade to upgrade): mask-rcnn==2.1 from git+https://github.com/matterport/Mask_RCNN.git in /usr/local/lib/python3.7/dist-packages\n",
            "Building wheels for collected packages: mask-rcnn\n",
            "  Building wheel for mask-rcnn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mask-rcnn: filename=mask_rcnn-2.1-cp37-none-any.whl size=56937 sha256=d9253064b876d0b2927a5d30e8e763afb4e28405ef468b5ac2210a1e0d1de207\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-hja055vi/wheels/c2/db/78/1af79db27d80d68b0eb94d95fda90b8b2c2f6e8862b13d5d01\n",
            "Successfully built mask-rcnn\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qnLdJWdQE8zD"
      },
      "source": [
        "# # Define input and output files\n",
        "# video_filename = \"pp1_s1_City_vidA.mp4\"\n",
        "# gaze_filename = \"GD_pp1_s1_City_vidA.csv\"\n",
        "\n",
        "# # Import the model and tensorflow, checking that it is running with the GPU\n",
        "# from MRCNN_Functions import *\n",
        "# %tensorflow_version 1.x\n",
        "# import tensorflow as tf\n",
        "# check_gpu()\n",
        "\n",
        "# !pip install git+https://github.com/matterport/Mask_RCNN.git\n",
        "\n",
        "# import mrcnn\n",
        "# from mrcnn.config import Config\n",
        "# from mrcnn import model as modellib\n",
        "# from mrcnn import visualize\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "auqlnZ6nw2xx",
        "cellView": "form"
      },
      "source": [
        "#@title Filename Definition - IGNORE\n",
        "# Define the names of the input and output files. \n",
        " \n",
        "# Test the Network with user input\n",
        "# p_num = int(input(\"Participant Number: \")) # CHANGED FOR ROOM VIDEO\n",
        "p_num = input(\"Participant Number: \")\n",
        "vid_num = str(input(\"Video: \"))\n",
        "sesh_num = int(input(\"Participant Session Number: \"))\n",
        "environment = str(input(\"Environment (City, Nature): \"))\n",
        "date = str(input(\"Date (e.g. 27042020): \"))\n",
        "green_check = input(\"Do you want the code to stop and allow for a greenery check (yes or no): \")\n",
        " \n",
        "# Define path to where all resources are stored in the dirive\n",
        "path = r\"/content/gdrive/My Drive/Gaze_Detection_System\"\n",
        " \n",
        " \n",
        "input_video_filename = \"pp{}_s{}_{}_vid{}.mp4\".format(p_num, sesh_num, environment, vid_num) #,date)   # 'pp4_vidA_s1_N_0308.mp4' # Name of video input\n",
        "output_video_filename = \"pp{}_vid{}_s{}_{}_{}.mp4\".format(p_num, vid_num, sesh_num, environment, date)  # 'pp4_vidA_s1_N_0308_output.mp4' # Name of video output\n",
        " \n",
        "gaze_filename = \"GD_pp{}_s{}_{}_vid{}.csv\".format(p_num, sesh_num, environment, vid_num) #, date)    # 'final_GD_pp4_vidA_s1_N_0308.csv' \n",
        " \n",
        "object_output_filename = \"mrcnn_obscores_pp{}_vid{}_s{}_{}_{}.csv\".format(p_num, vid_num, sesh_num, environment, date)         # \"mrcnn_obscores_pp4_vidA_s1_N_0308.csv\"\n",
        "GPVscores_output_filename =  \"mrcnn_GPVscores_pp{}_vid{}_s{}_{}_{}.csv\".format(p_num, vid_num, sesh_num, environment, date)                          # \"mrcnn_GPVscores_pp4_vidA_s1_N_0308.csv\"\n",
        "summary_scores_output_filename = \"mrcnn_summary_scores_pp{}_vid{}_s{}_{}_{}.csv\".format(p_num, vid_num, sesh_num, environment, date)                    # \"mrcnn_summary_scores_pp4_vidA_s1_N_0308.csv\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vmXkboMNFaxJ",
        "cellView": "form"
      },
      "source": [
        "#@title Install tensorflow and model - can ignore\n",
        "%tensorflow_version 1.x\n",
        "import tensorflow as tf\n",
        "\n",
        "#Check that tensorflow is running using the GPU\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))\n",
        "print(tf.__version__)\n",
        "\n",
        "# Grab the MRCNN model from github\n",
        "!pip install git+https://github.com/matterport/Mask_RCNN.git\n",
        "\n",
        "#Import the mrcnn model and all necessary accompanying files\n",
        "import mrcnn\n",
        "from mrcnn.config import Config\n",
        "from mrcnn import model as modellib\n",
        "from mrcnn import visualize\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7yOdQiGhCALs"
      },
      "source": [
        "# load in class label names\n",
        "def load_class_labels(path):\n",
        "  # load the class label names, one label per line\n",
        "  os.listdir(path)\n",
        "  CLASS_NAMES = open(os.path.join(path,\"coco_labels.txt\")).read().strip().split(\"\\n\")\n",
        "\n",
        "  # generate random (but visually distinct) colors for each class label\n",
        "  # (thanks to Matterport Mask R-CNN for the method!)\n",
        "  hsv = [(i / len(CLASS_NAMES), 1, 1.0) for i in range(len(CLASS_NAMES))]\n",
        "  COLORS = list(map(lambda c: colorsys.hsv_to_rgb(*c), hsv))\n",
        "  random.seed(42)\n",
        "  random.shuffle(COLORS)\n",
        "\n",
        "  return CLASS_NAMES, COLORS\n",
        "\n",
        "\n",
        "class SimpleConfig():\n",
        "\n",
        "\tNAME = \"coco_inference\" \n",
        "\tGPU_COUNT = 1\n",
        "\tIMAGES_PER_GPU = 1\n",
        "\tNUM_CLASSES = 81\n",
        "\n",
        "def configure_model(path, config, modellib):\n",
        "  weights_path =os.path.join(path, \"mask_rcnn_coco.h5\") \n",
        "\n",
        "  # initialize the Mask R-CNN model for inference and then load the\n",
        "  # weights\n",
        "  print(\"[INFO] loading Mask R-CNN model...\")\n",
        "  model = modellib.MaskRCNN(mode=\"inference\", config=config,\n",
        "  model_dir=os.getcwd())\n",
        "  model.load_weights(weights_path, by_name=True)\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 358
        },
        "id": "Okds1MTCKK8H",
        "outputId": "a05b8b83-73c2-4c4d-ab86-617172e7376d"
      },
      "source": [
        "## Load in all the necessary files for the MRCNN and initialise the network  \n",
        "\n",
        "# Get the class labels and associated colors\n",
        "path = os.getcwd()\n",
        "CLASS_NAMES, COLORS = load_class_labels(path)\n",
        "\n",
        "# initialize the inference configuration\n",
        "config = SimpleConfig()\n",
        "\n",
        "model = configure_model(path, config, modellib)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO] loading Mask R-CNN model...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-fe573aee8afb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSimpleConfig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfigure_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodellib\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-8-a396fd592c4a>\u001b[0m in \u001b[0;36mconfigure_model\u001b[0;34m(path, config, modellib)\u001b[0m\n\u001b[1;32m     29\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"[INFO] loading Mask R-CNN model...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m   model = modellib.MaskRCNN(mode=\"inference\", config=config,\n\u001b[0;32m---> 31\u001b[0;31m   model_dir=os.getcwd())\n\u001b[0m\u001b[1;32m     32\u001b[0m   \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mby_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/mrcnn/model.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, mode, config, model_dir)\u001b[0m\n\u001b[1;32m   1835\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_dir\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1836\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_log_dir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1837\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1838\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1839\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/mrcnn/model.py\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self, mode, config)\u001b[0m\n\u001b[1;32m   1846\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1847\u001b[0m         \u001b[0;31m# Image size must be dividable by 2 multiple times\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1848\u001b[0;31m         \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIMAGE_SHAPE\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1849\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mh\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m6\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m6\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m             raise Exception(\"Image size must be dividable by 2 at least 6 times \"\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'SimpleConfig' object has no attribute 'IMAGE_SHAPE'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wsno4GZDDsY5"
      },
      "source": [
        "\n",
        "# Add our own class_names to the class names list\n",
        "CLASS_NAMES.append('Background')\n",
        "CLASS_NAMES.append('Greenery')\n",
        "CLASS_NAMES.append('OOB')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uHlIc913YWO9"
      },
      "source": [
        "We then load in the gaze coordinates and convert into a list of integers so they can be compared to the Mask RCNN's output object coordinates"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hMjFhB75VFK3"
      },
      "source": [
        "raw_gaze = open(os.path.join(path, gaze_filename)).read().strip().split(\"\\n\")\n",
        "\n",
        "raw_gaze.pop(0) # remove the x and y\n",
        "GX = [round(float(pair.split(\",\")[0])) for pair in raw_gaze]\n",
        "GY = [round(float(pair.split(\",\")[1])) for pair in raw_gaze]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zDS2m2CpYlNk"
      },
      "source": [
        "Load in the video footage and define where the ouput video will be saved to. We also check that the number of frames in the video matches the number of coordinates listed in the gaze data. If there are more frames than coordinates, then the eye tarcker glitched and missed some frames during recording. If this numebr is too high then the gaze coordinates will be out of sync with the mrcnn object coordinates. If the eye tracker did not drop too many (< 10) frames, then the analysis can go ahead. If not, then the data may have to be collected again. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uqF4vDZTKL3y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3849864e-cdef-46e1-fbbd-e2edd30b30d4"
      },
      "source": [
        "vid_input = os.path.join(path, input_video_filename)\n",
        "vid_output = os.path.join(path, output_video_filename)\n",
        "\n",
        "#Initialise the video stream and pointer to output video file\n",
        "vs = cv2.VideoCapture(vid_input)\n",
        "writer = None\n",
        "\n",
        "# Obtain the number of frames in the video\n",
        "\n",
        "prop = cv2.cv.CV_CAP_PROP_FRAME_COUNT if imutils.is_cv2() else cv2.CAP_PROP_FRAME_COUNT\n",
        "total = int(vs.get(prop))\n",
        "print(\"[INFO] {} total frames in video\".format(total))\n",
        "\n",
        "\n",
        "# Check that the number of frames in which the eye tracker failed to collect any data\n",
        "\n",
        "dropped_frames = total - len(GX)\n",
        "\n",
        "if dropped_frames > 0 and dropped_frames < 10:\n",
        "  [GX.append(0) for i in range(0,dropped_frames)]\n",
        "  [GY.append(0) for i in range(0,dropped_frames)]\n",
        "  print('[INFO] The eye tracker dropped {} frames. {} 0s have been added to each gaze coordinate list.'.format(dropped_frames, dropped_frames))\n",
        "\n",
        "elif dropped_frames > 10:\n",
        "   raise Exception('The number of dropped frames is too high: {}'.format(dropped_frames))\n",
        "\n",
        "else:\n",
        "  print(\"[INFO] All good. The eye tracker didn't drop any frames.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO] 211 total frames in video\n",
            "[INFO] All good. The eye tracker didn't drop any frames.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "REkZ942GaNjk"
      },
      "source": [
        "USER ACTIONA REQUIRED\n",
        "\n",
        "If the greenery_check was sat to 'True' then the code will stop and present images of what the greenery system will output. 4 image pairs will be presented. The top image in the pair shows the original frame, the bottom image shows what the greenery mask identifies as green (white pixels = green pixels).Change the two key variables (Greenery_value_A and B) depending on whether the system is detecting too much green (e.g if detecting light/dark colours as green) or too little green. Once the greenery check is complete, change the greenery check value to FALSE and re-run the code (the ain for loop will not work if done after the greenery check without re-importing the video). Be sure to input the final greenery values when prompted in the next cell. \n",
        "\n",
        "If the greenery check is set to False, the following cell will still prompt the user to insert greenery values. Good default values are: \n",
        "\n",
        "Greenery_value_A = 50\n",
        "Greenery_Value_B = 90"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iVtatxO_COM-"
      },
      "source": [
        "if green_check == 'yes':\n",
        "\n",
        "  # Alter the below two values until the greenery mask image is identifying greenery in the frame image at an optimum rate\n",
        "  greenery_value_A = 80\n",
        "  greenery_value_B = 90\n",
        "  for frame_num in range(0, round(total/4)):\n",
        "    grabbed, frame = vs.read()\n",
        "    frame_list = [1, round((total/4)*0.25), round((total/4)*0.5), round((total/4)*0.75), round((total/4))]\n",
        "    if frame_num in frame_list:\n",
        "      hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n",
        "      test_gmask = cv2.inRange(hsv,(30, greenery_value_A, 20), (greenery_value_B, 255, 255) )\n",
        "      cv2_imshow(frame)\n",
        "      cv2.waitKey(0)\n",
        "      cv2_imshow(test_gmask)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cn6rnqS7aDdV"
      },
      "source": [
        "When prompted, insert the final greenery values. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qqHh0aA7lW0T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d9fd10b-31b7-4301-ed77-12dc54db46d7"
      },
      "source": [
        "greenery_value_A = int(input('Insert final greenery value for Value A here: '))\n",
        "greenery_value_B = int(input('Insert final greenery value for Value B here: '))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Insert final greenery value for Value A here: 80\n",
            "Insert final greenery value for Value B here: 90\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jj8_zr8Hf8GA"
      },
      "source": [
        "Below we define the functions used in the main system for loop."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-pKfHKeKsD8z"
      },
      "source": [
        "def draw_mrcnn_output(frame, startX, startY, endX, endY, color, label, score):\n",
        "  cv2.rectangle(frame, (startX, startY), (endX, endY), color, 2)\n",
        "  text = \"{}: {:.3f}\".format(label, score)\n",
        "  y = startY - 10 if startY - 10 > 10 else startY + 10\n",
        "  cv2.putText(frame, text, (startX, y), cv2.FONT_HERSHEY_SIMPLEX,\n",
        "    0.6, color, 2)   \n",
        "\n",
        "def apply_green_overlay(frame, gmask, greenery_score):\n",
        "    overlay = frame.copy()\n",
        "    grindex = np.where(gmask == 255) # Find the pixels that are green\n",
        "    overlay[grindex[0][:],grindex[1][:],0] = 250 # Change these pixels in the overlay - make blue value high\n",
        "    overlay[grindex[0][:],grindex[1][:],1] = 20 # Green value low\n",
        "    overlay[grindex[0][:],grindex[1][:],2] = 20 # Red value low\n",
        "    alpha = 0.6\n",
        "    cv2.addWeighted(overlay, alpha, frame, 1-alpha, 0 , frame)\n",
        "\n",
        "    if len(grindex[0][:]):\n",
        "      temp_greenery_score = (len(grindex[0][:]) / (720*1280)) * 100\n",
        "      greenery_score.append(temp_greenery_score) #  This stores the percentage of the given frame that was identified as green. \n",
        "\n",
        "    return greenery_score\n",
        "\n",
        " \n",
        "def identify_inframe_objects(GY, GX, count, mask, inframe_gaze_checklist, confidence_list, inframe_object_loc_X,\\\n",
        "                          inframe_object_loc_Y, startX, startY):\n",
        " \n",
        "\n",
        "  if GY[count] > 0 and GY[count] < 720 and GX[count] > 0 and GX[count] < 1280: # If the gaze fell within the headview camera's boundaries\n",
        "    if mask[GY[count],GX[count]]: # == True:\n",
        "      #label_winner = classID\n",
        "      inframe_gaze_checklist.append(CLASS_NAMES[classID]) # inframe_gaze_checklist is the objects appearing in the given frame\n",
        "      confidence_list.append(2)\n",
        "      inframe_object_loc_X.append(startX)\n",
        "      inframe_object_loc_Y.append(startY)\n",
        "\n",
        "    else:\n",
        "      #label_winner = 'Background'\n",
        "      inframe_gaze_checklist.append(0)\n",
        "      confidence_list.append(0)\n",
        "      inframe_object_loc_X.append(0)\n",
        "      inframe_object_loc_Y.append(0)\n",
        "      \n",
        "  else: # If gaze did not fall within the head view camera boundaries then add 'outofbounds' to the gazed_upon_object_list list\n",
        "    label_winner = 'Out Of Bounds'\n",
        "    inframe_gaze_checklist.append('OOB')\n",
        "    confidence_list.append(0)\n",
        "    inframe_object_loc_X.append(0)\n",
        "    inframe_object_loc_Y.append(0)\n",
        "\n",
        "  return inframe_gaze_checklist, confidence_list, inframe_object_loc_X, inframe_object_loc_Y\n",
        "\n",
        "\n",
        "def get_gazed_upon_object(inframe_gaze_checklist, confidence_list, inframe_object_loc_X, inframe_object_loc_Y, gazed_upon_object_list, confidence, gmask):\n",
        "\n",
        "  gazed_upon_index = [i for i, e in enumerate(inframe_gaze_checklist) if e != 0] # gazed_upon_index is the index of the winning classID (if there is one)\n",
        "\n",
        "\n",
        "  if len(gazed_upon_index) > 0: # If there is a single winning object then add the winner to the gazed_upon_object_list list    \n",
        "    new_champ = inframe_gaze_checklist[gazed_upon_index[0]]  \n",
        "    current_startX = inframe_object_loc_X[gazed_upon_index[0]] # This defines the current location of the gazed upon object\n",
        "    current_startY = inframe_object_loc_Y[gazed_upon_index[0]]\n",
        "    \n",
        "    if new_champ == 59: # if it's a potted plant, then mark as green\n",
        "      gazed_upon_object_list.append('Greenery')\n",
        "    else: # If the recognised object is not a potted plant, then add that classID to the gazed_upon_object_list list\n",
        "      gazed_upon_object_list.append(new_champ)\n",
        "\n",
        "  elif len(gazed_upon_index) == 0: # If no winning object was found, then check if green is being looked at\n",
        "    \n",
        "    if GY[count] > 0 and GY[count] < 720 and GX[count] > 0 and GX[count] < 1280: # If the gaze fell within the headview camera's boundaries \n",
        "      if gmask[GY[count],GX[count]] == 255: # If the gaze coords falls on a green area (The mask is flipped - so is GY,GX)\n",
        "        new_champ = 'Greenery'\n",
        "        gazed_upon_object_list.append('Greenery') # 82 is greenery\n",
        "        confidence.append(0)\n",
        "        \n",
        "      elif gmask[GY[count],GX[count]] == 0:\n",
        "        new_champ = 'Background'\n",
        "        gazed_upon_object_list.append('Background')\n",
        "        confidence.append(0)\n",
        "\n",
        "    else:\n",
        "      new_champ = 'OOB'\n",
        "\n",
        "  return gazed_upon_object_list, confidence, new_champ\n",
        "\n",
        "\n",
        "def overlay_label(CLASS_NAMES, new_champ, frame):\n",
        "  # Draw the Text on top\n",
        "  font = cv2.FONT_HERSHEY_SIMPLEX \n",
        "  org = (50, 50)       \n",
        "  fontScale = 2      \n",
        "  color = (0, 0, 255)  # Red     \n",
        "  thickness = 2      \n",
        "  # winner_label = CLASS_NAMES[new_champ] # text to draw\n",
        "  image = cv2.putText(frame, new_champ, org, font,  \n",
        "                    fontScale, color, thickness, cv2.LINE_AA) \n",
        "  return image\n",
        "\n",
        "\n",
        "def overlay_gaze_cursor(GX, GY, count, frame):\n",
        "  center_coordinates = (GX[count], GY[count]) \n",
        "  radius = 30 \n",
        "  color = (0, 0, 255) \n",
        "  thickness = -1\n",
        "    \n",
        "  # Using cv2.circle() method \n",
        "  # Draw a circle of red color of thickness -1 px \n",
        "  image = cv2.circle(frame, center_coordinates, radius, color, thickness) \n",
        "  return image\n",
        "\n",
        "def print_user_info(count, total):\n",
        "  if count == 8:\n",
        "    elap = (end - start)\n",
        "    print(\"[INFO] single frame took {:.4f} seconds\".format(elap))\n",
        "    print(\"[INFO] estimated total time to finish: {:.4f}\".format((elap * (total/4))))    \n",
        "  elif count == round(total/8): #round(quartal/2):\n",
        "    print(\"[INFO] Halfway!\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KDPr-rnvJD7G"
      },
      "source": [
        "Below are the functions for grphing the data. First for the graph showing what is gazed upon for each frame of footage, then for the heatmap"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RfTBgr2h7JB3"
      },
      "source": [
        "# Generate the graphs\n",
        "# First normalise data to make it a normal list\n",
        "def normalise_data(data):\n",
        "  data_set_edit = []\n",
        "  for i in range(0,len(data)):\n",
        "    data_set_edit.append(data[i])# [0])\n",
        "  return data_set_edit\n",
        "\n",
        "def prepare_for_graphing(data_set, unique_cats): # Prepare data for graphing\n",
        "  unique_cats_2 = unique_cats\n",
        "  data_set_2 = data_set\n",
        "  graph_list = []\n",
        "  for category in unique_cats:\n",
        "    graph_list.append([i for i, e in enumerate(data_set) if e == category])\n",
        "\n",
        "\n",
        "  GU_object_graphing_dict = {}\n",
        "  for i in range(0,len(unique_cats)):\n",
        "    GU_object_graphing_dict.update( {unique_cats[i]: get_broken_bar_list(graph_list[i])} )\n",
        "    \n",
        "  return GU_object_graphing_dict\n",
        "\n",
        "# Function to get final list for the broken bar chart\n",
        "def get_broken_bar_list(gaze_object_list):\n",
        "  output_list = []\n",
        "  for i in gaze_object_list:\n",
        "    output_list.append((i,1))\n",
        "\n",
        "  return output_list\n",
        "\n",
        "def generate_graph(GU_object_graphing_dict, unique_cats, system_data, ax):\n",
        "\n",
        "  y_tick_labels = [] # For the Ytick Labels\n",
        "  iterations = []\n",
        "\n",
        "  color_list = ['purple', 'olive', 'brown', 'cyan', 'pink']*15\n",
        "  color_dict = {}\n",
        "  count = 0\n",
        "  # Get colour for the graphing\n",
        "  for i in unique_cats:\n",
        "    if i == 'Greenery':\n",
        "      color_dict.update( {i: 'green'} )\n",
        "    elif i == 'Person':\n",
        "      color_dict.update( {i:'red'})\n",
        "    elif i == 'Vehicle':\n",
        "      color_dict.update( {i:'orange'})\n",
        "    elif i == 'Background':\n",
        "      color_dict.update( {i:'blue'})\n",
        "    else:\n",
        "      color_dict.update( {i: color_list[count]} )\n",
        "    count += 1\n",
        "\n",
        "\n",
        "  count = 0\n",
        "  for i in GU_object_graphing_dict:\n",
        "    count += 1\n",
        "    ax.broken_barh(GU_object_graphing_dict[i], ((count*5), 5), facecolors='tab:{}'.format(color_dict[i])) \n",
        "\n",
        "    y_tick_labels.append(count*5+2.5)\n",
        "    iterations.append(count*5)\n",
        "\n",
        "\n",
        "  ax.set_ylim(5, len(unique_cats)*5+5)\n",
        "  ax.set_xlim(0, len(system_data))\n",
        "  ax.set_xlabel('Frame Number')\n",
        "  ax.set_ylabel('Gazed Upon Object')\n",
        "\n",
        "  # if title:\n",
        "  #   ax.set_title('Objects Gazed At For Each Frame Of Footage As Generated By The Novel System and The Human Coder'.format(coder_identity))\n",
        "\n",
        "  ax.set_yticks(y_tick_labels)\n",
        "  ax.set_yticklabels(unique_cats)\n",
        "  ax.grid(False)\n",
        "\n",
        "  iterations = [10,15,20,25,30] # Add in lines around bars\n",
        "  for i in iterations:\n",
        "    ax.axhline(y=i,linewidth=1, color='gray', alpha = 0.3)\n",
        "\n",
        "  return ax\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dqlb5wozkarH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58b376a5-f695-49fa-8eff-fe68c4d5bf1c"
      },
      "source": [
        "# Define the lists that will be built in the following while loop\n",
        "gazed_upon_object_list = [] # List of objects that were being looked at for each frame\n",
        "confidence = [] # List saying whether the gaze is in the mask/just in the box\n",
        "green_list = []\n",
        "greenery_score = []\n",
        "persons_in_video = []\n",
        "vehicles_in_video = []\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Begin the processing for loop. This finds the coordinates and labels for each object and computes a greenery mask. \n",
        "It compares the gaze coordinates with the object coordinates/greenery mask to determine what is being looked at for each frame.\n",
        "The final frame-by-frame list is stored in the 'gazed_upon_object_list' and the 'average_persons_per_frame' and \n",
        "'average_vehicles_per_frame' are an show the average number of persons/vehicles in each frame. The average_greenery_score\n",
        "is the average percentage of pixels in each frame that were tagged as green.\n",
        "\"\"\"\n",
        "now = datetime.now() #Print the current time for timing checks\n",
        "current_time = now.strftime(\"%H:%M:%S\")\n",
        "print('[INFO] Current Time =', current_time)\n",
        "\n",
        "for count in range(0, total):\n",
        "  \n",
        "  start = time.time()\n",
        "  # read the next frame from the file\n",
        "  (grabbed, frame) = vs.read()\n",
        "  \n",
        "  if (count % 4) == 0: # We only process 1 in 4 frames (to save time). Delete this if statement to process all frames\n",
        "    hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n",
        "    gmask = cv2.inRange(hsv,(30, greenery_value_A, 20), (greenery_value_B, 255, 255) )\n",
        "    r = model.detect([frame], verbose=0)[0] # verbose tells us whether we want to see the output or not\n",
        "\n",
        "    inframe_gaze_checklist = []\n",
        "    inframe_labels = []\n",
        "    confidence_list = []\n",
        "    inframe_object_loc_X = []\n",
        "    inframe_object_loc_Y = []\n",
        "    persons_in_frame = 0\n",
        "    vehicles_in_frame = 0\n",
        "\n",
        "    for object in range(0, r[\"rois\"].shape[0]): # Loop over class labels\n",
        "      # extract the class ID and mask for the current detection, then\n",
        "      # grab the color to visualize the mask (in BGR format)\n",
        "      classID = r[\"class_ids\"][object]\n",
        "      mask = r[\"masks\"][:, :, object]\n",
        "      color = COLORS[classID][::-1]\n",
        "\n",
        "      # visualize the pixel-wise mask of the object\n",
        "      frame = visualize.apply_mask(frame, mask, color, alpha=0.5)\n",
        "\n",
        "      # extract the bounding box information, class ID, label, predicted\n",
        "      # probability, and visualization color\n",
        "      (startY, startX, endY, endX) = r[\"rois\"][object]\n",
        "      classID = r[\"class_ids\"][object]\n",
        "      label = CLASS_NAMES[classID]\n",
        "      score = r[\"scores\"][object]\n",
        "      color = [int(c) for c in np.array(COLORS[classID]) * 255]\n",
        "      inframe_labels.append(classID)\n",
        "\n",
        "      if classID == 1: # If it's a person, add one to the person count list\n",
        "        persons_in_frame += 1\n",
        "      elif classID == 3 or classID == 4 or classID == 6 or classID == 8: # If object is a car, bus, motorbike or truck, then add that to the list\n",
        "        vehicles_in_frame +=1\n",
        "\n",
        "      draw_mrcnn_output(frame, startX, startY, endX, endY, color, label, score)\n",
        "\n",
        "      # Find labels and coords for all objects in a frame\n",
        "      inframe_gaze_checklist, confidence_list, inframe_object_loc_X, inframe_object_loc_Y = identify_inframe_objects(GY, GX, count, mask, inframe_gaze_checklist, confidence_list, inframe_object_loc_X,\\\n",
        "                          inframe_object_loc_Y, startX, startY)\n",
        "\n",
        "      # Now combine the frames together to produce the output video     \n",
        "      if writer is None:\n",
        "        fourcc = cv2.VideoWriter_fourcc(*\"MJPG\") # Initialises the video writer\n",
        "        writer = cv2.VideoWriter(vid_output, fourcc, 10,\n",
        "          (frame.shape[1], frame.shape[0]), True) # Vid output is the path to the drive folder where the output video will go  \n",
        "    \n",
        "    greenery_score = apply_green_overlay(frame, gmask, greenery_score) # Find percentage of frame that is green greenery_score and apply greenery mask to the frame\n",
        "    persons_in_video.append(persons_in_frame) # Add the v and p counts so that index 1 gives the number of people/vehicles in the first frame, same for index 2 etc....\n",
        "    vehicles_in_video.append(vehicles_in_frame)\n",
        "    \n",
        "    \n",
        "    gazed_upon_index = [i for i, e in enumerate(inframe_gaze_checklist) if e != 0] # gazed_upon_index is the index of the winning classID (if there is one)\n",
        "\n",
        "    gazed_upon_object_list, confidence, new_champ = get_gazed_upon_object(inframe_gaze_checklist, confidence_list, inframe_object_loc_X, inframe_object_loc_Y, gazed_upon_object_list, confidence, gmask)\n",
        "\n",
        "    overlay_label(CLASS_NAMES, new_champ, frame) # !!!!!! Change to not being == to image\n",
        "    overlay_gaze_cursor(GX, GY, count, frame)\n",
        "\n",
        "    end = time.time()\n",
        "    writer.write(frame)\n",
        "\n",
        "  print_user_info(count, total)\n",
        "\n",
        "# release the file pointers\n",
        "print(\"[INFO] cleaning up...\")\n",
        "writer.release()\n",
        "vs.release()\n",
        "\n",
        " "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO] Current Time = 12:56:31\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "[INFO] single frame took 0.5438 seconds\n",
            "[INFO] estimated total time to finish: 28.6858\n",
            "[INFO] Halfway!\n",
            "[INFO] cleaning up...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "la_QGtB0m0ln"
      },
      "source": [
        "Below downloads the final list showing which object is being gazed upon for each frame of footage"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0DnVM8HsJAmw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "78b1f370-6c2e-4734-c803-55cf049d7317"
      },
      "source": [
        "average_persons_per_frame = (sum(persons_in_video)/len(persons_in_video))\n",
        "average_vehicles_per_frame = (sum(vehicles_in_video)/len(vehicles_in_video))\n",
        "average_greenery_score = (sum(greenery_score)/len(greenery_score))\n",
        "key_features_scores_list = [int(round(total/4)), average_persons_per_frame, average_vehicles_per_frame, average_greenery_score]\n",
        "\n",
        "key_feature_scores_df = pd.DataFrame(columns=['Feature', 'Score'])\n",
        "key_feature_scores_df['Feature'] = ['Num of processed frames','Persons per frame', 'Vehicles per frame', 'Greenery per frame (% of pixels)']\n",
        "key_feature_scores_df['Score'] = key_features_scores_list\n",
        "\n",
        "key_feature_scores_df.to_csv(summary_scores_output_filename, index = False)\n",
        "files.download(summary_scores_output_filename)\n",
        "\n",
        "GU_list_df = pd.DataFrame(gazed_upon_object_list) \n",
        "GU_list_df.to_csv(object_output_filename, index = False)\n",
        "files.download(object_output_filename)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_dcc29dfd-11c9-4eb5-bc01-2b2d4ce18db1\", \"mrcnn_summary_scores_pp1_vidA_s1_City_0101.csv\", 139)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_40d19d2b-293e-49ad-8634-55ebc98c04c9\", \"mrcnn_obscores_pp1_vidA_s1_City_0101.csv\", 361)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 242
        },
        "id": "eWL-ufTT7fKq",
        "outputId": "9092d0e7-5b7f-4f85-c429-164a5f45725d"
      },
      "source": [
        "\n",
        "normal_system_data = normalise_data(gazed_upon_object_list)\n",
        "system_unique_categories = list(set(normal_system_data)) # Lists all unique objects appearing in GU list\n",
        "GU_object_graphing_dict = prepare_for_graphing(normal_system_data, system_unique_categories)\n",
        "\n",
        "#Initiate Graph\n",
        "fig, plotter = plt.subplots(1, figsize=(8,3), sharex = True, gridspec_kw = {'hspace' : 0.05})\n",
        "plt.rc('font', family='sans serif')\n",
        "plt.rc('xtick', labelsize='x-small')\n",
        "plt.rc('ytick', labelsize='x-small')\n",
        "\n",
        "ax = generate_graph(GU_object_graphing_dict, system_unique_categories, gazed_upon_object_list, plotter)\n",
        "\n",
        "fig.savefig('Comparison_Fig_Video_{}.svg'.format(date), format='svg', dpi=1200) # , dpi=1200)\n",
        "files.download('Comparison_Fig_Video_{}.svg'.format(date))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_e74052a2-ff45-4729-8c38-c6c85c5b8043\", \"Comparison_Fig_Video_0101.svg\", 38552)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 576x216 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p1Z72AoqyRTj"
      },
      "source": [
        "Below is old processing stuff. Can ignore for now. The first cell is a working basic version of the system. Above is this version add a feature that overlays the currently gazed upon item label on top. Below writes every 4 frames, so is quicker to complete. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        },
        "id": "O5HfAPcPtaVY",
        "outputId": "d3697fc8-6bb8-4e40-a833-fa6f0b758cda"
      },
      "source": [
        "# It's deleting as we go so the legnth of spareGX is getting smaller so will eventually be out of range\n",
        "heatmap, xedges, yedges = np.histogram2d(GX, GY, bins=50)\n",
        "extent = [xedges[0], xedges[-1], yedges[0], yedges[-1]]\n",
        "\n",
        "#plt.clf()\n",
        "hetmap_fig = plt.imshow(heatmap.T, extent=extent, origin='lower')\n",
        "plt.title('Heat map displaying distribution of gaze')\n",
        "\n",
        "plt.savefig('heatmap_fig.pdf')\n",
        "files.download('heatmap_fig.pdf')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GiA5a6gRd333"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}